{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "8BYp6rJmMshM"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "  **Assignment 2**\n",
        "  \n",
        "  Name: Tao He\n",
        "  \n",
        "  Discussion Partner: Yifan Zhang"
      ],
      "metadata": {
        "id": "j0y3zMnJzyVL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment 2**: (One week, due in Gradescope at midnight 9/28 with same grace period and late policy as in PS 1)\n",
        "\n",
        "In this assignment, we are going to try building language models with \n",
        "the data we collected from various sources.\n",
        "\n",
        "In the first half, we are going to analyze our twitter data with NLTK. \n",
        "There are several tasks we would like to you to finish during this process:\n",
        "\n",
        "1. Preprocess the raw twitter data and make them into a format that\n",
        "language models in NLTK can train with.\n",
        "2. Train uni-gram, bi-gram and tri-gram models with Add-one smoothing.\n",
        "3. Compute perplexity to evaluate our language models based on different test sets.\n",
        "4. Generate new sentences with our language models based on the trained data.\n",
        "5. Perform sentiment analysis on our scraped data.\n",
        "\n",
        "In the following sections, we are going to provide a code template to allow you\n",
        "to complete them step by step.\n",
        "\n",
        "Here's a [general guide](https://www.kaggle.com/code/alvations/n-gram-language-model-with-nltk/notebook) of how to build language model with NLTK, please refer to this guide from time to time to see what you missed.\n",
        "\n",
        "Please submit this code with your implementaton and outputs. **Please indicate which students, if any, you consulted with as you completed this assignment.** "
      ],
      "metadata": {
        "id": "XF8Ns8RL2_Kq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, please go back to the code of our first lab section. Scrape 10000 tweets\n",
        "which: football lang:en -has:mentions -has:links -is:retweet\n",
        "1. mentions 'fishing'\n",
        "2. is written in English\n",
        "3. does not mention any other twitter account (i.e. @).\n",
        "4. does not contain links.\n",
        "5. is not a re-tweet.\n",
        "\n",
        "Then, scrape 10000 tweets with the same rules above but mention 'football' instead of 'fishing' this time.\n",
        "\n",
        "Save the scraped tweets in separate files, one for 'fishing' tweets\n",
        "and one for 'football'."
      ],
      "metadata": {
        "id": "AIYKCaG57Hza"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1\n",
        "\n",
        "First, let's try loading our scraped data. To begin with, let's load our 'fishing' data. You may change the following\n",
        "function as necessary."
      ],
      "metadata": {
        "id": "MdfD899a8s0C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWQpFZWDzV1O",
        "outputId": "030e9695-fb00-4e2e-a56b-b7a318d48ea0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install tweepy\n",
        "!pip3 install tweepy --upgrade\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import tweepy\n",
        "client = tweepy.Client(bearer_token='AAAAAAAAAAAAAAAAAAAAAAl1hAEAAAAAra3%2BdGSwRPN3G2BB45Ea8fDSny4%3DXWlNMYMgowFpQWpqJGYjtjpZmUbYQxyrY3IZeRAddDCZm0S6Tm') # replace with your bearer token here."
      ],
      "metadata": {
        "id": "LC1qS46t0Alf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "999ac2f6-2381-4aa9-aebb-3a5c346a6807"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tweepy in /usr/local/lib/python3.7/dist-packages (3.10.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy) (1.15.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy) (3.2.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (1.24.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (1.7.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tweepy in /usr/local/lib/python3.7/dist-packages (3.10.0)\n",
            "Collecting tweepy\n",
            "  Downloading tweepy-4.10.1-py3-none-any.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 3.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests-oauthlib<2,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from tweepy) (1.3.1)\n",
            "Collecting requests<3,>=2.27.0\n",
            "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: oauthlib<4,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from tweepy) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.27.0->tweepy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.27.0->tweepy) (2022.6.15)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.27.0->tweepy) (2.1.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.27.0->tweepy) (1.24.3)\n",
            "Installing collected packages: requests, tweepy\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: tweepy\n",
            "    Found existing installation: tweepy 3.10.0\n",
            "    Uninstalling tweepy-3.10.0:\n",
            "      Successfully uninstalled tweepy-3.10.0\n",
            "Successfully installed requests-2.28.1 tweepy-4.10.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Language is English.\n",
        "# no mentions, no links, not re-tweet.\n",
        "query = 'fishing lang:en -has:mentions -has:links -is:retweet'\n",
        "tweets = list(tweepy.Paginator(client.search_recent_tweets, query=query, tweet_fields=['context_annotations', 'created_at'], max_results=100).flatten(limit=10000))\n",
        "print(\"{} tweets are collected.\".format(len(tweets)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCC-VSNi0YN0",
        "outputId": "e73f6838-b2f6-4f1b-e9cf-1d53d2a20d7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7957 tweets are collected.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "driveFolderDirectory = '/content/drive/My Drive/CS505/tweetsFishing.csv' \n",
        "pathToSave = driveFolderDirectory\n",
        "\n",
        "with open(pathToSave, 'w', newline='') as csvfile:\n",
        "  fieldnames = ['idx','tweetId', 'tweetText']\n",
        "  writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "  writer.writeheader()\n",
        "  for i,tweet in enumerate(tweets):\n",
        "    writer.writerow({'idx': i, 'tweetId': tweet.id,'tweetText': tweet.data['text']})"
      ],
      "metadata": {
        "id": "vt2jT0g21AYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loadTextFromCSV(csvPath):\n",
        "  tweetDict = {}\n",
        "  with open(csvPath, newline='') as csvfile:\n",
        "    reader = csv.DictReader(csvfile)\n",
        "    for row in reader:\n",
        "      tweetDict[int(row['idx'])] = row['tweetText']\n",
        "  return tweetDict\n",
        "\n",
        "# load fishing tweet data here:\n",
        "csvPathFish = \"/content/drive/My Drive/CS505/tweetsFishing.csv\"\n",
        "rawTweetDictFish = loadTextFromCSV(csvPathFish)\n",
        "\n",
        "# print tweet dictionary\n",
        "# print(\"rawTweetDictFish: \",rawTweetDictFish)"
      ],
      "metadata": {
        "id": "79qabVAI6_gK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2\n",
        "\n",
        "Next, we are going to pre-process texts with NLTK library.\n",
        "\n",
        "Install NLTK library if it's not in your Google Colab space.\n",
        "\n",
        "Download 'punkt' specifically for sentence segmentation."
      ],
      "metadata": {
        "id": "P8YsEnEyDaoS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "3JTVhy2Q9hjr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae586660-b537-47cc-e4bf-60b3fd61e20a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "T_SvPCIV8pLs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cc87884-4c08-472c-c5d8-952b4fea5348"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We preprocess our tweet data with the following steps:\n",
        "1. Split data into training and testing splits. (**80%** tweets for training and **20%** tweets for testing) \n",
        "2. Sentence segmentation/spliting.\n",
        "3. Lower-case all words in the sentences.\n",
        "4. Tokenization (you should use TweetTokenizer from NLTK.tokenize)\n",
        "5. Padding with begin-of-sentence and end-of-sentence symbols \n",
        "\n",
        "You may refer to the following materials:\n",
        "1. [Sentence segmentation](https://www.nltk.org/api/nltk.tokenize.html). \n",
        "2. [String Lower case](https://www.w3schools.com/python/ref_string_lower.asp).\n",
        "3. [Tweet Tokenization](https://www.nltk.org/api/nltk.tokenize.casual.html).\n",
        "4. [Padding tokenized sentences](https://www.nltk.org/_modules/nltk/lm/preprocessing.html). Particularly, please look at function 'padded_everygram_pipeline'."
      ],
      "metadata": {
        "id": "FNzigJJpER3C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Split Data"
      ],
      "metadata": {
        "id": "0teBBHzg2gXk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(rawTweetDataDict):\n",
        "    #Input: a dictionary contains raw tweet data scraped from Tweeter\n",
        "    #Output: two lists of tweet sentences (train/test), but each tweet sentence is\n",
        "    #     represented in the form of tokens.\n",
        "\n",
        "    rawTweetDataFrame = pd.DataFrame.from_dict(rawTweetDictFish, orient ='index') # first, change dic to dataframe\n",
        "\n",
        "    training_data     = rawTweetDataFrame.sample(frac=0.8, random_state=25) # set seed\n",
        "    testing_data      = rawTweetDataFrame.drop(training_data.index)\n",
        "\n",
        "    # convert to list output\n",
        "    train_list        = training_data.values.tolist()\n",
        "    test_list         = testing_data.values.tolist()\n",
        "\n",
        "    train = []\n",
        "    test  = [] \n",
        "    for i in train_list: \n",
        "      train.append(i[0])\n",
        "    for i in test_list: \n",
        "      test.append(i[0])\n",
        "\n",
        "    return(train, test)"
      ],
      "metadata": {
        "id": "58N88lCE1bnL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train, test = preprocess(rawTweetDictFish)"
      ],
      "metadata": {
        "id": "dm0guCLaXXW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Sentence Segmentation"
      ],
      "metadata": {
        "id": "aCuqmt93-3Nd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import sent_tokenize, word_tokenize\n",
        "def sentenceSegmentation(tweets):\n",
        "    #Input: a string of raw tweet\n",
        "    #Output: a list of strings, each element in the list is a segmented sentence\n",
        "\n",
        "  tweet_segment = []\n",
        "  for str in tweets: \n",
        "    txts = sent_tokenize(str)\n",
        "    for sent in txts: \n",
        "      tweet_segment.append(sent)\n",
        "  \n",
        "  return(tweet_segment)"
      ],
      "metadata": {
        "id": "ImYVhB65a7t4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = sentenceSegmentation(train)\n",
        "test  = sentenceSegmentation(test)"
      ],
      "metadata": {
        "id": "E-f3YRqEcic6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 String Lower Case"
      ],
      "metadata": {
        "id": "taZTYxztAYV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sentenceLowerCase(sentence):\n",
        "  #Input: a string of sentence\n",
        "  #Output: a string of sentence, but all words in the sentence are lower-cased.\n",
        "  return(sentence.lower())"
      ],
      "metadata": {
        "id": "grRWEvlmFP4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define lower list\n",
        "train_lower = []\n",
        "test_lower  = []\n",
        "\n",
        "for sent in train:\n",
        "  lower = sentenceLowerCase(sent)\n",
        "  # use code from the last assignment\n",
        "  lower = re.sub(r'[^a-zA-Z,.?!():;\\'\"]', ' ', lower)\n",
        "  lower = re.sub(r'\\n', '', lower)\n",
        "  train_lower.append(lower)\n",
        "\n",
        "for sent in test:\n",
        "  lower = sentenceLowerCase(sent)\n",
        "  lower = re.sub(r'[^a-zA-Z,.?!():;\\'\"]', ' ', lower)\n",
        "  lower = re.sub(r'\\n', '', lower)\n",
        "  test_lower.append(lower)"
      ],
      "metadata": {
        "id": "YZT8d_4Cdgco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4 Tokenization"
      ],
      "metadata": {
        "id": "NjVpJ1L3FGfd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.nltk.org/api/nltk.tokenize.casual.html\n",
        "\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "def sentenceTokenization(sentence):\n",
        "    #Input: a string of sentence\n",
        "    #Output: a list of tokens that belong to the sentence.\n",
        "    tknzr = TweetTokenizer()\n",
        "    return (tknzr.tokenize(sentence))"
      ],
      "metadata": {
        "id": "gVRO6J1HFQSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_token = []\n",
        "test_token  = []\n",
        "\n",
        "for sent in train_lower:\n",
        "  token = sentenceTokenization(sent)\n",
        "  train_token.append(token)\n",
        "\n",
        "for sent in test_lower:\n",
        "  token = sentenceTokenization(sent)\n",
        "  test_token.append(token)"
      ],
      "metadata": {
        "id": "J6Rg_-t6K_Gi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.5 Padding with begin-of-sentence and end-of-sentence symbols"
      ],
      "metadata": {
        "id": "W16-piJYHLev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.nltk.org/_modules/nltk/lm/preprocessing.html\n",
        "from nltk.util import pad_sequence\n",
        "\n",
        "# list(pad_sequence(text[0],\n",
        "#                   pad_left=True, left_pad_symbol=\"<s>\",\n",
        "#                   pad_right=True, right_pad_symbol=\"</s>\",\n",
        "#                   n=2)) # The n order of n-grams, if it's 2-grams, you pad once, 3-grams pad twice, etc. \n",
        "\n",
        "train_data = []\n",
        "test_data  = []\n",
        "\n",
        "for str in train_token:\n",
        "  token = list(pad_sequence(str,\n",
        "                            pad_left = True, left_pad_symbol = \"<s>\",\n",
        "                            pad_right = True, right_pad_symbol = \"</s>\",\n",
        "                            n = 2))\n",
        "  train_data.append(token)\n",
        "\n",
        "for str in test_token:\n",
        "  token = list(pad_sequence(str,\n",
        "                            pad_left = True, left_pad_symbol = \"<s>\",\n",
        "                            pad_right = True, right_pad_symbol = \"</s>\",\n",
        "                            n = 2))\n",
        "  test_data.append(token)"
      ],
      "metadata": {
        "id": "LM4V2hOSHUoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3\n",
        "\n",
        "Next, we build our n-gram model with our pre-processed data.\n",
        "First we need to pad our data with padded_everygram_pipeline. Then, we train our n-gram model with add-one smoothing using the corresponding functions in NLTK.\n",
        "\n",
        "Related materials:\n",
        "1. [Padding](https://www.nltk.org/_modules/nltk/lm/preprocessing.html)\n",
        "2. [N-gram Language Model](https://www.nltk.org/api/nltk.lm.html)\n",
        "\n",
        "Let's train unigram, bigram and trigram models with our train data split."
      ],
      "metadata": {
        "id": "8BYp6rJmMshM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.kaggle.com/code/alvations/n-gram-language-model-with-nltk\n",
        "from nltk.lm.preprocessing import pad_both_ends\n",
        "from nltk.lm.preprocessing import flatten\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "\n",
        "# Here's a template you may want to start with\n",
        "def trainNGramAddOneSmoothing(trainData,ngram):\n",
        "  # Input: a list of tweet sentences, each element is a list of tokens; n for ngram model\n",
        "  # Output: a n-gram model with add-one smoothing trained on your input data. \n",
        "  train, vocab = padded_everygram_pipeline(ngram, trainData)\n",
        "  return(train, vocab)\n",
        "\n",
        "unigram_train_data, unigram_train_vocab = trainNGramAddOneSmoothing(train_token, 1)\n",
        "bigram_train_data, bigram_train_vocab   = trainNGramAddOneSmoothing(train_token, 2)\n",
        "trigram_train_data, trigram_train_vocab = trainNGramAddOneSmoothing(train_token, 3)"
      ],
      "metadata": {
        "id": "sQrdUMTJN-6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 4\n",
        "\n",
        "Now we apply our analysis on the trained model. \n",
        "\n",
        "First, compute the average perplexity of your tri-gram model on the sentences of our test data.\n",
        "\n",
        "[How to compute perplexity](https://www.nltk.org/api/nltk.lm.html)\n",
        "\n",
        "Next, load the tweet data of 'football' instead, and compute the perplexity of your 'fishing' model on the football tweets. \n",
        "\n",
        "**Why is there a difference between the two perplexities, what causes it?**\n",
        "(Please answer in a text cell.)"
      ],
      "metadata": {
        "id": "HoxnRMdxOnSc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Average perplexity of the trigram model on the sentences of fishing data =  4.710520750462658.\n",
        "\n",
        "Average perplexity of the trigram model on the sentences of football data = 7.028510179325636.\n",
        "\n",
        "Although both of them are tri-gram model, we used different train data sets so that the models we got are definitely different. "
      ],
      "metadata": {
        "id": "M83kni8n5KFZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 tweet data of \"fishing\""
      ],
      "metadata": {
        "id": "jNuXwqSP380G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def computePerplexity(model,testData):\n",
        "    # Input: your model; the testing data\n",
        "    # Output: average perplexity of the model on your testing data.\n",
        "    test = list(trigrams(pad_both_ends(testData, n=3)))\n",
        "    perplexity = model.perplexity(test)\n",
        "    return(perplexity)"
      ],
      "metadata": {
        "id": "S9S4GvSjP_Nt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.lm import MLE\n",
        "from nltk.util import bigrams, trigrams\n",
        "\n",
        "trimodel_fishing = MLE(3)\n",
        "len(trimodel_fishing.vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RmtbMLBHtph",
        "outputId": "36dd54d6-baf3-459a-cc1b-ef70ee4f927c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trimodel_fishing.fit(trigram_train_data, trigram_train_vocab)\n",
        "print(trimodel_fishing.vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPEcjNuPIS1E",
        "outputId": "f0bc2745-e479-4d86-8dd2-2d952e1da74c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<Vocabulary with cutoff=1 unk_label='<UNK>' and 14461 items>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(trimodel_fishing.vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ZUFTtVxIaEe",
        "outputId": "5700a39b-4318-4dff-ecac-7771d9a88a5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14461"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score = []\n",
        "for sent in test_token:\n",
        "    score.append(computePerplexity(trimodel_fishing, sent))\n",
        "    \n",
        "# remove 'inf' term in score\n",
        "filtered_score = []\n",
        "\n",
        "for i in score:\n",
        "    if not ((np.isnan(i)) or (i == float('Inf')) or (i == float('-Inf'))):\n",
        "        filtered_score.append(i)\n",
        "        \n",
        "print('Average perplexity of the trigram model on the sentences of fishing data = ', sum(filtered_score)/len(filtered_score))\n",
        "# 1. the perplexity of your model on your testing data of 'fishing' tweets."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pp_jd6o-ILLn",
        "outputId": "3ce5376c-b901-4b1c-8383-e2eebb87d366"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average perplexity of the trigram model on the sentences of fishing data =  4.710520750462658\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 tweet data of \"football\""
      ],
      "metadata": {
        "id": "g8mzihnF4DZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Language is English.\n",
        "# no mentions, no links, not re-tweet.\n",
        "Query = 'football lang:en -has:mentions -has:links -is:retweet'\n",
        "football = list(tweepy.Paginator(client.search_recent_tweets, query=Query, tweet_fields=['context_annotations', 'created_at'], max_results=100).flatten(limit=10000))\n",
        "print(\"{} tweets are collected.\".format(len(football)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oxrp6nnW4HkY",
        "outputId": "0ac54db8-5085-4b3a-bf1f-e9daeba7a696"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000 tweets are collected.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "driveFolderDirectory = '/content/drive/My Drive/CS505/tweetsFootball.csv' \n",
        "pathToSave = driveFolderDirectory\n",
        "\n",
        "with open(pathToSave, 'w', newline='') as csvfile:\n",
        "  fieldnames = ['idx','tweetId', 'tweetText']\n",
        "  writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "  writer.writeheader()\n",
        "  for i,tweet in enumerate(football):\n",
        "    writer.writerow({'idx': i, 'tweetId': tweet.id,'tweetText': tweet.data['text']})"
      ],
      "metadata": {
        "id": "U6kL8DR3BCaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loadTextFromCSV(csvPath):\n",
        "  tweetDict = {}\n",
        "  with open(csvPath, newline='') as csvfile:\n",
        "    reader = csv.DictReader(csvfile)\n",
        "    for row in reader:\n",
        "      tweetDict[int(row['idx'])] = row['tweetText']\n",
        "  return tweetDict\n",
        "\n",
        "csvPathFootball = \"/content/drive/My Drive/CS505/tweetsFootball.csv\"\n",
        "rawTweetDictFootball = loadTextFromCSV(csvPathFootball)\n",
        "\n",
        "# print(\"rawTweetDictFootball: \",rawTweetDictFootball)"
      ],
      "metadata": {
        "id": "ao3LRYvmBDQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessFootball(rawTweetDataDict):\n",
        "    #Input: a dictionary contains raw tweet data scraped from Tweeter\n",
        "    #Output: two lists of tweet sentences (train/test), but each tweet sentence is\n",
        "    #     represented in the form of tokens.\n",
        "\n",
        "    rawTweetDataFrame = pd.DataFrame.from_dict(rawTweetDictFootball, orient ='index') # first, change dic to dataframe\n",
        "\n",
        "    training_data     = rawTweetDataFrame.sample(frac=0.8, random_state=25) # set seed\n",
        "    testing_data      = rawTweetDataFrame.drop(training_data.index)\n",
        "\n",
        "    # convert to list output\n",
        "    train_list        = training_data.values.tolist()\n",
        "    test_list         = testing_data.values.tolist()\n",
        "\n",
        "    train = []\n",
        "    test  = [] \n",
        "    for i in train_list: \n",
        "      train.append(i[0])\n",
        "    for i in test_list: \n",
        "      test.append(i[0])\n",
        "\n",
        "    return(train, test)"
      ],
      "metadata": {
        "id": "28gPSDRp-5Y_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split Data\n",
        "train_football, test_football = preprocessFootball(rawTweetDictFootball)\n",
        "# Sentence Segmentation\n",
        "train_football = sentenceSegmentation(train_football)\n",
        "test_football  = sentenceSegmentation(test_football)\n",
        "# String Lower Case\n",
        "football_train_lower = []\n",
        "football_test_lower  = []\n",
        "\n",
        "for sent in train_football:\n",
        "  lower = sentenceLowerCase(sent)\n",
        "  # use code from the last assignment\n",
        "  lower = re.sub(r'[^a-zA-Z,.?!():;\\'\"]', ' ', lower)\n",
        "  lower = re.sub(r'\\n', '', lower)\n",
        "  football_train_lower.append(lower)\n",
        "\n",
        "for sent in test_football:\n",
        "  lower = sentenceLowerCase(sent)\n",
        "  lower = re.sub(r'[^a-zA-Z,.?!():;\\'\"]', ' ', lower)\n",
        "  lower = re.sub(r'\\n', '', lower)\n",
        "  football_test_lower.append(lower)\n",
        "# Tokenization\n",
        "football_train_token = []\n",
        "football_test_token  = []\n",
        "\n",
        "for sent in football_train_lower:\n",
        "  token = sentenceTokenization(sent)\n",
        "  football_train_token.append(token)\n",
        "\n",
        "for sent in football_test_lower:\n",
        "  token = sentenceTokenization(sent)\n",
        "  football_test_token.append(token)"
      ],
      "metadata": {
        "id": "OJjlhPi6DMjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train unigram, bigram and trigram models with our train data split.\n",
        "f_unigram_train_data, f_unigram_train_vocab = trainNGramAddOneSmoothing(football_train_token, 1)\n",
        "f_bigram_train_data,  f_bigram_train_vocab   = trainNGramAddOneSmoothing(football_train_token, 2)\n",
        "f_trigram_train_data, f_trigram_train_vocab = trainNGramAddOneSmoothing(football_train_token, 3)"
      ],
      "metadata": {
        "id": "-rGKMpq7JYMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trimodel_football = MLE(3)\n",
        "trimodel_football.fit(f_trigram_train_data, f_trigram_train_vocab)\n",
        "\n",
        "football_score = []\n",
        "for sent in football_test_token:\n",
        "    football_score.append(computePerplexity(trimodel_football, sent))\n",
        "    \n",
        "# remove 'inf' term in score\n",
        "football_filtered_score = []\n",
        "\n",
        "for i in football_score:\n",
        "    if not ((np.isnan(i)) or (i == float('Inf')) or (i == float('-Inf'))):\n",
        "        football_filtered_score.append(i)\n",
        "        \n",
        "print('Average perplexity of the trigram model on the sentences of football data = ', sum(football_filtered_score)/len(football_filtered_score))\n",
        "# 2. the perplexity of your model on your data of 'football' tweets."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "248jEGY6On9I",
        "outputId": "795bdf31-374f-44d8-f10a-34588664c005"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average perplexity of the trigram model on the sentences of football data =  7.028510179325636\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 5\n",
        "\n",
        "Next, generate 10 tweets using each of your language models (unigram, bigram, trigram). The generated tweets needs to be in string format instead of tokens, also the string should be without padding.\n",
        "\n",
        "[Generate new sentences with your model.](https://www.nltk.org/api/nltk.lm.html)\n",
        "\n",
        "[Detokenize your generated sentences](https://www.nltk.org/howto/tokenize.html)"
      ],
      "metadata": {
        "id": "qk9I-CruVbHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unimodel_fishing = MLE(1)\n",
        "unimodel_fishing.fit(unigram_train_data, unigram_train_vocab)\n",
        "\n",
        "bimodel_fishing = MLE(2)\n",
        "bimodel_fishing.fit(bigram_train_data, bigram_train_vocab)"
      ],
      "metadata": {
        "id": "RVE4ee5CSSo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "def generateNewSentence(model, sent_length, randomSeed):\n",
        "  # Input: your model; random seed that get you different generated sentence\n",
        "  # Output: a new sentence generated by your model, but in a string format instead of tokens.\n",
        "  sent = model.generate(sent_length, random_seed = randomSeed) \n",
        "  de = TreebankWordDetokenizer().detokenize(sent)  # detokenize\n",
        "  sent_clr = re.sub(\"<s>|</s>\", \"\", de)\n",
        "  return(sent_clr)"
      ],
      "metadata": {
        "id": "0JBEXbXFTBxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make loops to generate 10 tweets for each of your model (unigram, bigram and trigram)\n",
        "unisentence = []\n",
        "bisentence  = []\n",
        "trisentence = []\n",
        "for i in range(10):\n",
        "  unisentence.append(generateNewSentence(unimodel_fishing, 10, i))\n",
        "  bisentence.append(generateNewSentence(bimodel_fishing, 10, i))\n",
        "  trisentence.append(generateNewSentence(trimodel_fishing, 10, i))"
      ],
      "metadata": {
        "id": "MVYbww6DapYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unisentence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8pwhOKZCshB",
        "outputId": "23cf5cf9-c754-4617-89e3-e53461d79f42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the shadow going close in games species ease i looking',\n",
              " \"add the shouts chest i'm he's occasion started:,\",\n",
              " 'when weeks . . the roc omfg engaging mechanism men',\n",
              " \"by it's fishing me my . (the clothing but\",\n",
              " 'but? friends and . fun up swimming sidearm boat',\n",
              " 'my s study wdym ryohgo use, household we now',\n",
              " 'straight the i combos! of husbands she fishing small',\n",
              " 'feel amp nymphing . it fishing . in, had',\n",
              " 'boy who a placement . carrying yourselves became no his',\n",
              " 'hook fishing agriculture thirst! im to . just more']"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bisentence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIj5sd4xIfox",
        "outputId": "fe36a692-44f1-44c6-92c9-1052d37515b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the same line after it is taking .  into',\n",
              " ' the same is kind of soft torture . ',\n",
              " 'went to assist because they said so future as others',\n",
              " 'a leopard chat on sunday!!  amp;',\n",
              " 'a cabin above all about his tiny tempor pet coyote',\n",
              " 'lest china often used to the announcers are while it',\n",
              " 'shots of military and \" liquorice .  dead of',\n",
              " 'books, not a lake? \" grandad, hunting',\n",
              " 'a while...it .  yours for our friend',\n",
              " 'for fishing . !  to be in something']"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trisentence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEdj7dkmIjI_",
        "outputId": "e650edea-91c5-4385-d9f0-9a6c42f5c6d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['suppose the same is true of everything in life.',\n",
              " '         ',\n",
              " 'we were bros...that looks very much related to',\n",
              " ' and i m not!!   ',\n",
              " \"  i didn't but i threw mine on anyway\",\n",
              " 'in the sea was calm, might have to learn',\n",
              " 'resume writing, fishing!     ',\n",
              " 'a cool front rolled in, after a few hours',\n",
              " \" we're about to become a second...maybe also\",\n",
              " 'edna collection .       ']"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 6\n",
        "\n",
        "Lastly, we want to perform sentiment analysis on our collected data.\n",
        "This time we will use VADER.\n",
        "\n",
        "Please check out the following material:\n",
        "\n",
        "[Sentiment analysis with VADER](https://www.geeksforgeeks.org/python-sentiment-analysis-using-vader/)\n",
        "\n",
        "Then do the following:\n",
        "\n",
        "1. Compute the ratios of positive and negative sentences in your collected data.\n",
        "2. Compute the average compound sentiment of the tweets for 'fishing' and 'football'. Are they generally positive or negative? (Answer in a text cell.)\n",
        "3. Compute the top 10 non stop words from positive tweets of 'fishing'. Please check out [here](https://www.geeksforgeeks.org/removing-stop-words-nltk-python/) to find out how to remove stop words in your sentences. The top 10 words shall also not include puncutations, including symbols like parenthesis ' \", or ...\n",
        "You can refer to [here](https://docs.python.org/3/library/string.html) to see how to exclude them (still there will be special cases, please remember to remove them as well.)"
      ],
      "metadata": {
        "id": "4PnKtMyZYGr7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1 compute the ratios of positive and negative sentences."
      ],
      "metadata": {
        "id": "wb3qYZCWwXvc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install VADER\n",
        "!pip install vaderSentiment"
      ],
      "metadata": {
        "id": "094lsQV8fCRd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23215bdd-0ff5-4a2e-b36d-a347589e2ea3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting vaderSentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 27.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from vaderSentiment) (2.28.1)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2022.6.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (1.24.3)\n",
            "Installing collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fishing_train_clr = []\n",
        "fishing_test_clr  = []\n",
        "\n",
        "for sent_list in train_token: \n",
        "    word_list = []\n",
        "    for word in sent_list: \n",
        "        word_clr = re.sub(r'[^A-Za-z]', '', word)\n",
        "        word_list.append(word_clr)\n",
        "    word_list = list(filter(None, word_list))\n",
        "    fishing_train_clr.append(word_list)\n",
        "        \n",
        "for sent_list in test_token: \n",
        "    word_list = []\n",
        "    for word in sent_list: \n",
        "        word_clr = re.sub(r'[^A-Za-z]', '', word)\n",
        "        word_list.append(word_clr)\n",
        "    word_list = list(filter(None, word_list))\n",
        "    fishing_test_clr.append(word_list)  \n",
        "\n",
        "\n",
        "football_train_clr = []\n",
        "football_test_clr  = []\n",
        "\n",
        "for sent_list in football_train_token: \n",
        "    word_list = []\n",
        "    for word in sent_list: \n",
        "        word_clr = re.sub(r'[^A-Za-z]', '', word)\n",
        "        word_list.append(word_clr)\n",
        "    word_list = list(filter(None, word_list))\n",
        "    football_train_clr.append(word_list)\n",
        "        \n",
        "for sent_list in football_train_token: \n",
        "    word_list = []\n",
        "    for word in sent_list: \n",
        "        word_clr = re.sub(r'[^A-Za-z]', '', word)\n",
        "        word_list.append(word_clr)\n",
        "    word_list = list(filter(None, word_list))\n",
        "    football_test_clr.append(word_list)\n",
        "\n",
        "\n",
        "fishing_clr = fishing_train_clr + fishing_test_clr\n",
        "football_clr = football_train_clr + football_test_clr"
      ],
      "metadata": {
        "id": "5ligtLjSo4uk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_lxABEu3c04",
        "outputId": "6f676e00-aa06-45a8-eb29-f75be1c564eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "# need to download 'stopwords' before using it.\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "print(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dh4KypcV4oUZ",
        "outputId": "643b153a-ab9b-4a0a-aa26-65943a7e0565"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def removeStopWords(sentence_list): \n",
        "    filtered_token_list = []\n",
        "    for sent in sentence_list:\n",
        "        filtered_token = []\n",
        "        for w in sent:\n",
        "            if w not in stop_words:\n",
        "                filtered_token.append(w)\n",
        "        filtered_token_list.append(filtered_token)\n",
        "    return(filtered_token_list)\n",
        "\n",
        "\n",
        "fishing_filtered_token  = removeStopWords(fishing_clr) # remove stop words\n",
        "fishing_filtered_token = [ele for ele in fishing_filtered_token if ele != []] # remove empty list\n",
        "football_filtered_token = removeStopWords(football_clr)\n",
        "football_filtered_token = [ele for ele in football_filtered_token if ele != []]"
      ],
      "metadata": {
        "id": "MYiEANMMzY0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/syp1997/CS505-NLP-2021-Spring/blob/main/homework2/homework2.ipynb\n",
        "\n",
        "def computeSentimentOfSentences(sentenceData):\n",
        "    senti_analyzer = SentimentIntensityAnalyzer()\n",
        "    avg_compound_score = []\n",
        "    i = 0\n",
        "    for sentence in sentenceData:\n",
        "    #     print(i)\n",
        "        compound_score = []\n",
        "        num_postive = 0\n",
        "        num_neutral = 0\n",
        "        num_negative = 0\n",
        "        pos_twitter = []\n",
        "        neu_twitter = []\n",
        "        neg_twitter = []\n",
        "        for text in sentence:\n",
        "            sentiment = senti_analyzer.polarity_scores(text)\n",
        "            score = sentiment['compound']\n",
        "            compound_score.append(score)\n",
        "            if score >= 0.05:\n",
        "                num_postive += 1\n",
        "                pos_twitter.append(text)\n",
        "            elif score < 0.05 and score > -0.05:\n",
        "                num_neutral += 1\n",
        "                neu_twitter.append(text)\n",
        "            elif score <= -0.05:\n",
        "                num_negative += 1\n",
        "                neg_twitter.append(text)\n",
        "            else:\n",
        "                print(\"no match!!\")\n",
        "        avg_score = (np.array(compound_score)).mean()\n",
        "        avg_compound_score.append(avg_score)\n",
        "        i = i+1\n",
        "    \n",
        "    sentenceArray = np.array(sentenceData, dtype=object)\n",
        "    avg_compound_scoreArray = np.array(avg_compound_score)\n",
        "    positive_index = np.where(avg_compound_scoreArray > 0)[0]\n",
        "    sentencePositive = list(sentenceArray[positive_index])\n",
        "    \n",
        "    return(avg_compound_score, sentencePositive)"
      ],
      "metadata": {
        "id": "AcUdGpUU3Le0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fishing_score, fishing_positive   = computeSentimentOfSentences(fishing_filtered_token)\n",
        "football_score, football_positive = computeSentimentOfSentences(football_filtered_token)\n",
        "fishing_score_array  = np.array(fishing_score)\n",
        "football_score_array = np.array(football_score)\n",
        "\n",
        "print(\"fishing_positive_ratio: {} ;  fishing_negative_ratio: {}\".format(round(len(np.where(fishing_score_array > 0)[0])/len(fishing_score_array), 3), round(len(np.where(fishing_score_array < 0)[0])/len(fishing_score_array), 3)))\n",
        "print(\"football_positive_ratio: {} ; football_negative_ratio: {}\".format(round(len(np.where(football_score_array > 0)[0])/len(football_score_array), 3), round(len(np.where(football_score_array < 0)[0])/len(football_score_array), 3)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lz31NM1R3OOx",
        "outputId": "61768fae-9aac-4d41-99af-027f77e65ca8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fishing_positive_ratio: 0.353 ;  fishing_negative_ratio: 0.199\n",
            "football_positive_ratio: 0.407 ; football_negative_ratio: 0.19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.2 Compute the average compound sentiment of the tweets for 'fishing' and 'football'. Are they generally positive or negative? (Answer in a text cell.)"
      ],
      "metadata": {
        "id": "JIgfak1Awj_z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "They generally positive."
      ],
      "metadata": {
        "id": "UuXTaGNl6vHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"average sentiment compound of fishing: \", sum(fishing_score)/len(fishing_score))\n",
        "print(\"average sentiment compound of football: \", sum(football_score)/len(football_score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00Az_LnX6LP2",
        "outputId": "1f22e28f-1cfa-4b6b-b63f-a8c3d12302b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average sentiment compound of fishing:  0.017127030263975405\n",
            "average sentiment compound of football:  0.019431180960325943\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.3 Compute the top 10 non stop words from positive tweets of 'fishing'."
      ],
      "metadata": {
        "id": "p_scvwoMws0Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "uniques = []\n",
        "for sentence in fishing_positive:\n",
        "    for word in sentence:\n",
        "        if word not in uniques:\n",
        "            uniques.append(word)\n",
        "            \n",
        "counts = []\n",
        "for unique in uniques:\n",
        "    count = 0\n",
        "    for sentence in fishing_positive: \n",
        "        for word in sentence:\n",
        "            if word == unique:\n",
        "                count += 1\n",
        "    counts.append((count, unique))"
      ],
      "metadata": {
        "id": "9wRXrFPK_tt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counts.sort()\n",
        "counts.reverse()\n",
        "print('top 10 words in positive tweets: ')\n",
        "for i in range(min(10, len(counts))):\n",
        "    count, word = counts[i]\n",
        "    print('%s: %d;' % (word, count))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7iD70LrANnC",
        "outputId": "a29ffe2c-bed6-4605-c8e5-032f182977cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "top 10 words in positive tweets: \n",
            "fishing: 3149;\n",
            "like: 834;\n",
            "go: 398;\n",
            "good: 334;\n",
            "want: 325;\n",
            "love: 303;\n",
            "fish: 265;\n",
            "im: 231;\n",
            "one: 217;\n",
            "would: 194;\n"
          ]
        }
      ]
    }
  ]
}